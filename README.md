# PISanitizer â€“ LLM Prompt Injection Defense

This project explores prompt injection attacks on Large Language Models
and implements a lightweight rule-based prompt sanitization defense.

The goal is to understand how prompt injection works, evaluate existing
defenses, and extend them with improved explainability and evaluation.

## Setup
1. Create a virtual environment
2. Install requirements
3. Run example scripts

(Details will be expanded as the project progresses.)
(Basic skelton subject to change as needed)